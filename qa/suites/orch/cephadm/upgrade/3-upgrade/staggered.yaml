tasks:
- cephadm.shell:
    env: [sha1]
    mon.a:
      # setup rgw
      - radosgw-admin realm create --rgw-realm=r --default
      - radosgw-admin zonegroup create --rgw-zonegroup=default --master --default
      - radosgw-admin zone create --rgw-zonegroup=default --rgw-zone=z --master --default
      - radosgw-admin period update --rgw-realm=r --commit
      - ceph orch apply rgw foo --realm r --zone z --placement=2 --port=8000
      # setup iscsi
      - ceph osd pool create foo
      - rbd pool init foo
      - ceph orch apply iscsi foo u p
      - sleep 180
      - ceph config set mon mon_warn_on_insecure_global_id_reclaim false --force
      - ceph config set mon mon_warn_on_insecure_global_id_reclaim_allowed false --force
      - ceph config set global log_to_journald false --force
      # get some good info on the state of things pre-upgrade. Useful for debugging
      - ceph orch ps
      - ceph versions
      - ceph -s
      - ceph orch ls
      # collect the target id for the container we are upgrading to
      - export TARGET_ID="$(ceph orch upgrade check --image quay.ceph.io/ceph-ci/ceph:$sha1 | jq -r '.target_id')"
      - echo $TARGET_ID
      # doing staggered upgrade requires mgr daemons being on a version that contains the staggered upgrade code
      # until there is a stable version that contains it, we can test by manually upgrading a mgr daemon
      - ceph orch daemon redeploy "mgr.$(ceph mgr dump -f json | jq .standbys | jq .[] | jq -r .name)" --image quay.ceph.io/ceph-ci/ceph:$sha1
      - ceph orch ps --refresh
      - sleep 180
      # gather more possible debugging info
      - ceph orch ps
      - ceph versions
      - ceph -s
      - ceph health detail
      # verify we have upgraded exactly 1 of the 2 mgr daemons to the new image id
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="mgr") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==1'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="mgr") | select(.container_image_id!=$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==1'
      - ! cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ceph mgr fail
      - sleep 180
      # now try upgrading the other mgr
      - ceph orch daemon redeploy "mgr.$(ceph mgr dump -f json | jq .standbys | jq .[] | jq -r .name)" --image quay.ceph.io/ceph-ci/ceph:$sha1
      - ceph orch ps --refresh
      - sleep 180
      # gather more possible debugging info
      - ceph orch ps
      - ceph versions
      - ceph health detail
      - ceph -s
      - ceph mgr fail
      - sleep 180
      # gather more debugging info
      - ceph orch ps
      - ceph versions
      - ceph -s
      - ceph health detail
      # now that both mgrs should have been redeployed with the new version, so should find 2 daemons
      # when matching against mgr daemons on the correct image id
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="mgr") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==2'
      - ceph mgr fail
      - sleep 180
      # debugging info
      - ceph orch ps
      - ceph orch ls
      - ceph versions
      # to make sure mgr daemons upgrade is fully completed, including being deployed by a mgr on new new version
      # also serves as an early failure if manually upgrading the mgrs failed as --daemon-types won't be recognized
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1 --daemon-types mgr
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do ceph orch ps ; ceph versions ; ceph orch upgrade status ; sleep 30 ; done
      # verify 2 mgr daemons both on the new container image id
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="mgr") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==2'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ! cat out.json | jq -e '.[] | .container_image_id' | grep -v $TARGET_ID
      # verify non-mgr daemons are still on old image id to make sure --daemon-types was respected
      - ! ceph orch ps --format json | jq -e '.[] | select(.daemon_type!="mgr") | .container_image_id' | grep $TARGET_ID
      # check that exactly two daemons have been upgraded to the new image (our 2 mgr daemons)
      - ceph orch upgrade check quay.ceph.io/ceph-ci/ceph:$sha1 | jq -e '.up_to_date | length == 2'
      - ceph orch upgrade status
      - ceph health detail
      # upgrade only the mons on one of the two hosts
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1 --daemon-types mon --hosts $(ceph orch ps | grep mgr.x | awk '{print $2}')
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do ceph orch ps ; ceph versions ; ceph orch upgrade status ; sleep 30 ; done
      - ceph orch ps
      # verify exactly 1 off the 2 mon daemons was upgraded
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="mon") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==1'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="mon") | select(.container_image_id!=$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==1'
      - ! cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ceph orch upgrade status
      - ceph health detail
      # upgrade mons on the other hosts
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1 --daemon-types mon --hosts $(ceph orch ps | grep mgr.y | awk '{print $2}')
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do ceph orch ps ; ceph versions ; ceph orch upgrade status ; sleep 30 ; done
      - ceph orch ps
      # verify all mons now on same version and version hash matches what we are upgrading to
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="mon") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==2'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ! cat out.json | jq -e '.[] | .container_image_id' | grep -v $TARGET_ID
      # verify exactly 5 daemons are now upgraded (2 mgrs, 3 mons)
      - ceph orch upgrade check quay.ceph.io/ceph-ci/ceph:$sha1 | jq -e '.up_to_date | length == 5'
      - ceph orch upgrade status
      - ceph health detail
      # upgrade exactly 2 osd daemons
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1 --daemon-types osd --limit 2
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do ceph orch ps ; ceph versions ; ceph orch upgrade status ; sleep 30 ; done
      - ceph orch ps
      # verify exactly 2 of the 8 OSDs were upgraded
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="osd") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==2'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      # verify exactly 7 daemons have been upgraded (2 mgrs, 3 mons, 2 osds)
      - ceph orch upgrade check quay.ceph.io/ceph-ci/ceph:$sha1 | jq -e '.up_to_date | length == 7'
      - ceph orch upgrade status
      - ceph health detail
      # upgrade one more osd
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1 --daemon-types crash,osd --limit 1
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do ceph orch ps ; ceph versions ; ceph orch upgrade status ; sleep 30 ; done
      - ceph orch ps
      # verify 3 osd daemons have been upgraded
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="osd") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==3'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      # verify now 8 daemons have been upgraded
      - ceph orch upgrade check quay.ceph.io/ceph-ci/ceph:$sha1 | jq -e '.up_to_date | length == 8'
      # upgrade the rest of the osds
      # use this opportunity to check we can set osd flags properly
      - ceph orch upgrade status
      - ceph health detail
      # make sure noout is listed as a flag to be set as that is what we'll test with
      - ceph config get mgr mgr/cephadm/upgrade_osd_flags | grep noout
      # upgrade osds and crash daemons.
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1 --daemon-types crash,osd
      # wait for upgrade to be started and in progress to check for osd flags
      # To test noout being set during upgrade, want to loop here until either the upgrade completes,
      # fails with an error, or noout is set, but in the noout case, we need to do something to mark
      # that that was the condition that we stopped looping on. Doing that here by having
      # it create a file whose existence we can check for once the loop is over
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do if ceph osd dump -f json | jq '.flags_set' | grep noout; then touch saw_noout.txt; break; else echo "no noout yet"; fi; sleep 1; done
      - ls | grep saw_noout
      # wait for upgrade to complete
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do ceph orch ps ; ceph versions ; ceph orch upgrade status ; sleep 30 ; done
      - ceph orch ps
      # verify noout was unset once upgrade completed
      - if ceph osd dump -f json | jq '.flags_set' | grep noout; then (exit 1); else (exit 0); fi
      # verify all 8 osds are on the new container image id
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="osd") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==8'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ! cat out.json | jq -e '.[] | .container_image_id' | grep -v $TARGET_ID
      - ceph orch upgrade status
      - ceph health detail
      # upgrade the rgw daemons using --services
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1 --services rgw.foo
      - while ceph orch upgrade status | jq '.in_progress' | grep true && ! ceph orch upgrade status | jq '.message' | grep Error ; do ceph orch ps ; ceph versions ; ceph orch upgrade status ; sleep 30 ; done
      - ceph orch ps
      # verify all 2 rgw daemons were upgraded
      - ceph orch ps --format json | jq --arg TARGET_ID "$TARGET_ID" -e '.[] | select(.daemon_type=="rgw") | select(.container_image_id==$TARGET_ID)' | jq -s > out.json
      - cat out.json | jq -e 'length==2'
      - cat out.json | jq -e '.[] | .container_image_id' | grep $TARGET_ID
      - ! cat out.json | jq -e '.[] | .container_image_id' | grep -v $TARGET_ID
      - ceph orch upgrade status
      - ceph health detail
      # run upgrade one more time with no filter parameters to make sure anything left gets upgraded
      - ceph orch upgrade start --image quay.ceph.io/ceph-ci/ceph:$sha1
